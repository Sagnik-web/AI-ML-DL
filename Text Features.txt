# Basic

import pandas as pd
import re
from nltk.corpus import stopwords

# Sample DataFrame
df = pd.DataFrame({
    'text': [
        'I love programming in Python!',
        'Machine learning is amazing.',
        'Text data needs preprocessing.'
    ]
})

# Basic preprocessing function
def preprocess_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    return text

df['cleaned_text'] = df['text'].apply(preprocess_text)

# Remove stop words
stop_words = set(stopwords.words('english'))
df['tokenized'] = df['cleaned_text'].apply(lambda x: [word for word in x.split() if word not in stop_words])

print(df[['text', 'cleaned_text', 'tokenized']])








# Bag of Words (BoW)
from sklearn.feature_extraction.text import CountVectorizer

# Initialize the CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the text data
X_bow = vectorizer.fit_transform(df['cleaned_text'])
print("Bag of Words representation:")
print(X_bow.toarray())
print(vectorizer.get_feature_names_out())







# Term Frequency-Inverse Document Frequency (TF-IDF)

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the text data
X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text'])
print("TF-IDF representation:")
print(X_tfidf.toarray())
print(tfidf_vectorizer.get_feature_names_out())







# Word Embeddings
from gensim.models import Word2Vec

# Train a Word2Vec model
model = Word2Vec(sentences=df['tokenized'], vector_size=50, window=2, min_count=1, workers=4)

# Get the vector for a specific word
vector = model.wv['python']
print("Vector for the word 'python':")
print(vector)







# Sentence Embeddings
from sentence_transformers import SentenceTransformer

# Load the Sentence-BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Generate sentence embeddings
embeddings = model.encode(df['cleaned_text'].tolist())
print("Sentence embeddings:")
print(embeddings)









# N-grams
from sklearn.feature_extraction.text import CountVectorizer

# Initialize the CountVectorizer with n-grams
vectorizer = CountVectorizer(ngram_range=(1, 2))  # Unigrams and bigrams

# Fit and transform the text data
X_ngrams = vectorizer.fit_transform(df['cleaned_text'])
print("N-grams representation:")
print(X_ngrams.toarray())
print(vectorizer.get_feature_names_out())


















